{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you submit this notebook, make sure everything runs as expected in the local test cases. \n",
    "Please, paste the solution to the designed cell and do not change anything else.\n",
    "\n",
    "Also, please, leave your first and last names below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FirstName = \"\"\n",
    "LastName = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54c6398a05b33742cb4853125e3f7359",
     "grade": false,
     "grade_id": "cell-ac8fc52d8a39ccb4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    \"\"\"\n",
    "    Basically, you can think of a module as of a something (black box) \n",
    "    which can process `input` data and produce `ouput` data.\n",
    "    This is like applying a function which is called `forward`: \n",
    "        \n",
    "        output = module.forward(input)\n",
    "    \n",
    "    The module should be able to perform a backward pass: to differentiate the `forward` function. \n",
    "    More, it should be able to differentiate it if is a part of chain (chain rule).\n",
    "    The latter implies there is a gradient from previous step of a chain rule. \n",
    "    \n",
    "        gradInput = module.backward(input, gradOutput)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 0: Initialization (0.01 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "495ee7017854a686dcb711999a798d41",
     "grade": true,
     "grade_id": "cell-3473b7b6ffd64d07",
     "locked": true,
     "points": 0.01,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change this cell\n",
    "import sys\n",
    "\n",
    "import numpy\n",
    "import numpy as np\n",
    "import unittest\n",
    "\n",
    "import collections\n",
    "import pickle\n",
    "import io\n",
    "\n",
    "\n",
    "numpy.variance = numpy.var\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Linear layer (0.04 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "43c55aeef47a245f9747b49a5e1d0ea0",
     "grade": true,
     "grade_id": "cell-e3503c286039ec55",
     "locked": true,
     "points": 0.04,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "batch_size, n_in, n_out = 2, 3, 4\n",
    "for d in data['test_Linear']:\n",
    "    # layers initialization\n",
    "    custom_layer = Linear(n_in, n_out)\n",
    "    custom_layer.W = d['init_w']\n",
    "    custom_layer.b = d['init_w_b']\n",
    "\n",
    "    layer_input = d['input']\n",
    "    next_layer_grad = d['next_layer_grad']\n",
    "\n",
    "    # 1. check layer output\n",
    "    custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "    gt_output = d['output']\n",
    "    assert np.allclose(gt_output, custom_layer_output, atol=1e-6)\n",
    "\n",
    "    # 2. check layer input grad\n",
    "    custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "    gt_layer_grad = d['grad_output']\n",
    "    assert np.allclose(gt_layer_grad, custom_layer_grad, atol=1e-6)\n",
    "\n",
    "    # 3. check layer parameters grad\n",
    "    custom_layer.accGradParameters(layer_input, next_layer_grad)\n",
    "    weight_grad = custom_layer.gradW\n",
    "    bias_grad = custom_layer.gradb\n",
    "    torch_weight_grad = d['w_grad']\n",
    "    torch_bias_grad = d['b_grad']\n",
    "    assert np.allclose(torch_weight_grad, weight_grad, atol=1e-6)\n",
    "    assert np.allclose(torch_bias_grad, bias_grad, atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Softmax (0.1 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "853ead287715eea03766c5a047432ee3",
     "grade": true,
     "grade_id": "cell-e2c4124a6f815118",
     "locked": true,
     "points": 0.1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "batch_size, n_in = 2, 4\n",
    "for d in data['test_SoftMax']:\n",
    "    # layers initialization\n",
    "    custom_layer = SoftMax()\n",
    "\n",
    "    layer_input = d['input']\n",
    "    next_layer_grad = d['next_layer_grad']\n",
    "\n",
    "    # 1. check layer output\n",
    "    custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "    gt_output = d['output']\n",
    "    assert np.allclose(gt_output, custom_layer_output, atol=1e-5)\n",
    "\n",
    "    # 2. check layer input grad\n",
    "    custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "    assert np.allclose(d['grad_output'], custom_layer_grad, atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3: LogSoftMax (0.1 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "913f891bb4fb0e38fb454912eb02f1c6",
     "grade": true,
     "grade_id": "cell-69473387a23d8dff",
     "locked": true,
     "points": 0.1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "batch_size, n_in = 2, 4\n",
    "for d in data['test_LogSoftMax']:\n",
    "    # layers initialization\n",
    "    custom_layer = LogSoftMax()\n",
    "\n",
    "    layer_input = d['input']\n",
    "    next_layer_grad = d['next_layer_grad']\n",
    "\n",
    "    # 1. check layer output\n",
    "    custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "    gt_output = d['output']\n",
    "    np.allclose(gt_output, custom_layer_output, atol=1e-6)\n",
    "\n",
    "    # 2. check layer input grad\n",
    "    custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "    gt_grad = d['grad_output']\n",
    "    np.allclose(gt_grad, custom_layer_grad, atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 4: BatchNormalization (0.1 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "956aa519dabe432152a0ede8a3752a56",
     "grade": true,
     "grade_id": "cell-e5f59af66e6c111b",
     "locked": true,
     "points": 0.1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "batch_size, n_in = 32, 16\n",
    "for d in data['test_BatchNormalization']:\n",
    "    # layers initialization\n",
    "    alpha = 0.9\n",
    "    custom_layer = BatchNormalization(alpha)\n",
    "    custom_layer.train()\n",
    "    init_moving_mean = d['init_moving_mean']\n",
    "    init_moving_variance = d['init_moving_variance']\n",
    "    custom_layer.moving_mean = init_moving_mean\n",
    "    custom_layer.moving_variance = init_moving_variance\n",
    "\n",
    "    layer_input = d['input']\n",
    "    next_layer_grad = d['next_layer_grad']\n",
    "\n",
    "    # 1. check layer output\n",
    "    custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "    gt_output = d['output']\n",
    "    np.allclose(gt_output, custom_layer_output, atol=1e-6)\n",
    "\n",
    "    # 2. check layer input grad\n",
    "    custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "    gt_grad = d['grad_output']\n",
    "    # please, don't increase `atol` parameter, it's garanteed that you can implement batch norm layer\n",
    "    # with tolerance 1e-5\n",
    "    np.allclose(gt_grad, custom_layer_grad, atol=1e-5)\n",
    "\n",
    "    # 3. check moving mean\n",
    "    gt_running_mean = d['gt_moving_mean']\n",
    "    gt_running_var = d['gt_moving_var']\n",
    "    np.allclose(custom_layer.moving_mean, gt_running_mean)\n",
    "    # we don't check moving_variance because pytorch uses slightly different formula for it:\n",
    "    # it computes moving average for unbiased variance (i.e var*N/(N-1))\n",
    "    #self.assertTrue(np.allclose(custom_layer.moving_variance, torch_layer.running_var.numpy()))\n",
    "\n",
    "    # 4. check evaluation mode\n",
    "    custom_layer.moving_variance = gt_running_var\n",
    "    custom_layer.evaluate()\n",
    "    custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "    eval_output = d['eval_output']\n",
    "    np.allclose(eval_output, custom_layer_output, atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 5: Sequential (0.1 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8073cadf9a101e69fc84567477431581",
     "grade": true,
     "grade_id": "cell-1a9c1e3609ed9aab",
     "locked": true,
     "points": 0.1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "batch_size, n_in = 2, 4\n",
    "for d in data['test_Sequential']:\n",
    "    # layers initialization\n",
    "    alpha = 0.9\n",
    "    custom_layer = Sequential()\n",
    "    bn_layer = BatchNormalization(alpha)\n",
    "    bn_layer.moving_mean = d['init_moving_mean']\n",
    "    bn_layer.moving_variance = d['init_moving_var']\n",
    "\n",
    "    custom_layer.add(bn_layer)\n",
    "    scaling_layer = ChannelwiseScaling(n_in)\n",
    "    scaling_layer.gamma = d['init_gamma']\n",
    "    scaling_layer.beta = d['init_beta']\n",
    "    custom_layer.add(scaling_layer)\n",
    "    custom_layer.train()\n",
    "\n",
    "    layer_input = d['input']\n",
    "    next_layer_grad = d['next_layer_grad']\n",
    "\n",
    "    # 1. check layer output\n",
    "    custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "    assert np.allclose(d['gt_output'], custom_layer_output, atol=1e-5)\n",
    "\n",
    "    # 2. check layer input grad\n",
    "    custom_layer_grad = custom_layer.backward(layer_input, next_layer_grad)\n",
    "    assert np.allclose(d['gt_grad'], custom_layer_grad, atol=1e-5)\n",
    "\n",
    "    # 3. check layer parameters grad\n",
    "    weight_grad, bias_grad = custom_layer.getGradParameters()[1]\n",
    "    torch_weight_grad = d['gt_weight_grad']\n",
    "    torch_bias_grad = d['gt_bias_grad']\n",
    "    assert np.allclose(torch_weight_grad, weight_grad, atol=1e-5)\n",
    "    assert np.allclose(torch_bias_grad, bias_grad, atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 6: Dropout (0.075 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2a5deb8722e44de6c892bc73a80df812",
     "grade": true,
     "grade_id": "cell-1ddb0377b6c68deb",
     "locked": true,
     "points": 0.15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "batch_size, n_in = 2, 4\n",
    "for _ in range(100):\n",
    "    # layers initialization\n",
    "    p = np.random.uniform(0.3, 0.7)\n",
    "    layer = Dropout(p)\n",
    "    layer.train()\n",
    "\n",
    "    layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "    next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "\n",
    "    # 1. check layer output\n",
    "    layer_output = layer.updateOutput(layer_input)\n",
    "    assert np.all(np.logical_or(np.isclose(layer_output, 0),\n",
    "                                np.isclose(layer_output*(1.-p), layer_input)))\n",
    "\n",
    "    # 2. check layer input grad\n",
    "    layer_grad = layer.updateGradInput(layer_input, next_layer_grad)\n",
    "    assert np.all(np.logical_or(np.isclose(layer_grad, 0),\n",
    "                                np.isclose(layer_grad*(1.-p), next_layer_grad)))\n",
    "\n",
    "    # 3. check evaluation mode\n",
    "    layer.evaluate()\n",
    "    layer_output = layer.updateOutput(layer_input)\n",
    "    assert np.allclose(layer_output, layer_input)\n",
    "\n",
    "    # 4. check mask\n",
    "    p = 0.0\n",
    "    layer = Dropout(p)\n",
    "    layer.train()\n",
    "    layer_output = layer.updateOutput(layer_input)\n",
    "    assert np.allclose(layer_output, layer_input)\n",
    "\n",
    "    p = 0.5\n",
    "    layer = Dropout(p)\n",
    "    layer.train()\n",
    "    layer_input = np.random.uniform(5, 10, (batch_size, n_in)).astype(np.float32)\n",
    "    next_layer_grad = np.random.uniform(5, 10, (batch_size, n_in)).astype(np.float32)\n",
    "    layer_output = layer.updateOutput(layer_input)\n",
    "    zeroed_elem_mask = np.isclose(layer_output, 0)\n",
    "    layer_grad = layer.updateGradInput(layer_input, next_layer_grad)\n",
    "    assert np.all(zeroed_elem_mask == np.isclose(layer_grad, 0))\n",
    "\n",
    "    # 5. dropout mask should be generated independently for every input matrix element, not for row/column\n",
    "    batch_size, n_in = 1000, 1\n",
    "    p = 0.8\n",
    "    layer = Dropout(p)\n",
    "    layer.train()\n",
    "\n",
    "    layer_input = np.random.uniform(5, 10, (batch_size, n_in)).astype(np.float32)\n",
    "    layer_output = layer.updateOutput(layer_input)\n",
    "    assert np.sum(np.isclose(layer_output, 0)) != layer_input.size\n",
    "\n",
    "    layer_input = layer_input.T\n",
    "    layer_output = layer.updateOutput(layer_input)\n",
    "    assert np.sum(np.isclose(layer_output, 0)) != layer_input.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 7: LeakyReLU (0.05 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a6d3bf92fc934b4849cc25a685ee90e",
     "grade": true,
     "grade_id": "cell-da6c6ccf757a0621",
     "locked": true,
     "points": 0.05,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "batch_size, n_in = 2, 4\n",
    "for d in data['test_LeakyReLU']:\n",
    "    # layers initialization\n",
    "    slope = d['slope']\n",
    "    custom_layer = LeakyReLU(slope)\n",
    "\n",
    "    layer_input = d['layer_input']\n",
    "    next_layer_grad = d['next_layer_grad']\n",
    "\n",
    "    # 1. check layer output\n",
    "    custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "    assert np.allclose(d['gt_output'], custom_layer_output, atol=1e-6)\n",
    "\n",
    "    # 2. check layer input grad\n",
    "    custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "    assert np.allclose(d['gt_grad'], custom_layer_grad, atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 8: ELU (0.075 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7fef24e9868d89ff9237dd4dd980d19b",
     "grade": true,
     "grade_id": "cell-301cefbceb12834e",
     "locked": true,
     "points": 0.075,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "batch_size, n_in = 2, 4\n",
    "for d in data['test_ELU']:\n",
    "    # layers initialization\n",
    "    alpha = 1.0\n",
    "    custom_layer = ELU(alpha)\n",
    "\n",
    "    layer_input = d['layer_input']\n",
    "    next_layer_grad = d['next_layer_grad']\n",
    "\n",
    "    # 1. check layer output\n",
    "    custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "    assert np.allclose(d['gt_output'], custom_layer_output, atol=1e-6)\n",
    "\n",
    "    # 2. check layer input grad\n",
    "    custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "    assert np.allclose(d['gt_grad'], custom_layer_grad, atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 9: SoftPlus (0.1 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9f0e55c87fa6980548152a0a61b0702",
     "grade": true,
     "grade_id": "cell-f804d9bf6f70e562",
     "locked": true,
     "points": 0.1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "batch_size, n_in = 2, 4\n",
    "for d in data['test_SoftPlus']:\n",
    "    # layers initialization\n",
    "    custom_layer = SoftPlus()\n",
    "\n",
    "    layer_input = d['layer_input']\n",
    "    next_layer_grad = d['next_layer_grad']\n",
    "\n",
    "    # 1. check layer output\n",
    "    custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "    assert np.allclose(d['gt_output'], custom_layer_output, atol=1e-6)\n",
    "\n",
    "    # 2. check layer input grad\n",
    "    custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "    assert np.allclose(d['gt_grad'], custom_layer_grad, atol=1e-6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 10: ClassNLLCriterionUnstable (0.1 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae440874a6bb55cb01ffbf1b3954ec8a",
     "grade": true,
     "grade_id": "cell-76b1d3d075e9752b",
     "locked": true,
     "points": 0.1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "batch_size, n_in = 2, 4\n",
    "for d in data['test_ClassNLLCriterionUnstable']:\n",
    "    # layers initialization\n",
    "    custom_layer = ClassNLLCriterionUnstable()\n",
    "\n",
    "    layer_input = d['layer_input']\n",
    "    target = d['target']\n",
    "\n",
    "    # 1. check layer output\n",
    "    custom_layer_output = custom_layer.updateOutput(layer_input, target)\n",
    "    assert np.allclose(d['gt_output'], custom_layer_output, atol=1e-6)\n",
    "\n",
    "    # 2. check layer input grad\n",
    "    custom_layer_grad = custom_layer.updateGradInput(layer_input, target)\n",
    "    assert np.allclose(d['gt_grad'], custom_layer_grad, atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 11: ClassNLLCriterion (0.05 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de95a17ede1d7eb0260fe998d322df66",
     "grade": true,
     "grade_id": "cell-cbc87cb79435ffda",
     "locked": true,
     "points": 0.05,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "batch_size, n_in = 2, 4\n",
    "for d in data['test_ClassNLLCriterion']:\n",
    "    # layers initialization\n",
    "    custom_layer = ClassNLLCriterion()\n",
    "\n",
    "    layer_input = d['layer_input']\n",
    "    target = d['target']\n",
    "\n",
    "    # 1. check layer output\n",
    "    custom_layer_output = custom_layer.updateOutput(layer_input, target)\n",
    "    assert np.allclose(d['gt_output'], custom_layer_output, atol=1e-6)\n",
    "\n",
    "    # 2. check layer input grad\n",
    "    custom_layer_grad = custom_layer.updateGradInput(layer_input, target)\n",
    "    assert np.allclose(d['gt_grad'], custom_layer_grad, atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 12: Adam (0.1 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02ae6923f9ab677df706edaa2ace8f26",
     "grade": true,
     "grade_id": "cell-31b7bd19c0cfd8f5",
     "locked": true,
     "points": 0.1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "state = {}\n",
    "config = {'learning_rate': 1e-3, 'beta1': 0.9, 'beta2':0.999, 'epsilon':1e-8}\n",
    "variables = [[np.arange(10).astype(np.float64)]]\n",
    "gradients = [[np.arange(10).astype(np.float64)]]\n",
    "adam_optimizer(variables, gradients, config, state)\n",
    "assert np.allclose(state['m'][0], np.array([0. , 0.1, 0.2, 0.3, 0.4, 0.5,\n",
    "                                                     0.6, 0.7, 0.8, 0.9]))\n",
    "assert np.allclose(state['v'][0], np.array([0., 0.001, 0.004, 0.009, 0.016, 0.025,\n",
    "                                                     0.036, 0.049, 0.064, 0.081]))\n",
    "assert state['t'] == 1\n",
    "assert np.allclose(variables[0][0], np.array([0., 0.999, 1.999, 2.999, 3.999, 4.999,\n",
    "                                                       5.999, 6.999, 7.999, 8.999]))\n",
    "adam_optimizer(variables, gradients, config, state)\n",
    "assert np.allclose(state['m'][0], np.array([0., 0.19, 0.38, 0.57, 0.76, 0.95, 1.14,\n",
    "                                                     1.33, 1.52, 1.71]))\n",
    "assert np.allclose(state['v'][0], np.array([0., 0.001999, 0.007996, 0.017991,\n",
    "                                                     0.031984, 0.049975, 0.071964, 0.097951,\n",
    "                                                     0.127936, 0.161919]))\n",
    "assert state['t'] == 2\n",
    "assert np.allclose(variables[0][0], np.array([0., 0.998, 1.998, 2.998, 3.998, 4.998,\n",
    "                                                       5.998, 6.998, 7.998, 8.998]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 13: Conv2d (0.35 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "698d34a5c5f8b4a29cc1366be93be511",
     "grade": true,
     "grade_id": "cell-b63afa10ee124617",
     "locked": true,
     "points": 0.35,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "batch_size, n_in, n_out = 2, 3, 4\n",
    "h,w = 5,6\n",
    "kern_size = 3\n",
    "for d in data['test_Conv2d']:\n",
    "    # layers initialization\n",
    "    custom_layer = Conv2d(n_in, n_out, kern_size)\n",
    "    custom_layer.W = d['init_w']\n",
    "    custom_layer.b = d['init_b']\n",
    "\n",
    "    layer_input = d['input']\n",
    "    next_layer_grad = d['next_layer_grad']\n",
    "\n",
    "    # 1. check layer output\n",
    "    custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "    assert np.allclose(d['gt_output'], custom_layer_output, atol=1e-6)\n",
    "\n",
    "    # 2. check layer input grad\n",
    "    custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "    assert np.allclose(d['gt_grad'], custom_layer_grad, atol=1e-6)\n",
    "\n",
    "    # 3. check layer parameters grad\n",
    "    custom_layer.accGradParameters(layer_input, next_layer_grad)\n",
    "    weight_grad = custom_layer.gradW\n",
    "    bias_grad = custom_layer.gradb\n",
    "    assert np.allclose(d['gt_weight_grad'], weight_grad, atol=1e-6, )\n",
    "    assert np.allclose(d['gt_bias_grad'], bias_grad, atol=1e-6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 13: MaxPool2d (0.15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dab96dd2b8af6e265d6c635512e7964e",
     "grade": true,
     "grade_id": "cell-1392d1511ad4da77",
     "locked": true,
     "points": 0.15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "batch_size, n_in = 2, 3\n",
    "h,w = 4,6\n",
    "kern_size = 2\n",
    "for d in data['test_MaxPool2d']:\n",
    "    # layers initialization\n",
    "    custom_layer = MaxPool2d(kern_size)\n",
    "\n",
    "    layer_input = d['input']\n",
    "    next_layer_grad = d['next_layer_grad']\n",
    "\n",
    "    # 1. check layer output\n",
    "    custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "    assert np.allclose(d['gt_output'], custom_layer_output, atol=1e-6)\n",
    "\n",
    "    # 2. check layer input grad\n",
    "    custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "    assert np.allclose(d['gt_grad'], custom_layer_grad, atol=1e-6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
