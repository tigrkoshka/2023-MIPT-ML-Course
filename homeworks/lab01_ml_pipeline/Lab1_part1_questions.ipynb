{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWHlIX2WhI56"
   },
   "source": [
    "*Credits: materials from this notebook belong to YSDA [Practical DL](https://github.com/yandexdataschool/Practical_DL) course. Special thanks for making them available online.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UnvFMzpDhI5-"
   },
   "source": [
    "# Lab assignment â„–1, part 1\n",
    "\n",
    "This lab assignment consists of several parts. You are supposed to make some transformations, train some models, estimate the quality of the models and explain your results.\n",
    "\n",
    "Several comments:\n",
    "* Don't hesitate to ask questions, it's a good practice.\n",
    "* No private/public sharing, please. The copied assignments will be graded with 0 points.\n",
    "* Blocks of this lab will be graded separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnak4F97hI5_"
   },
   "source": [
    "## 1. Matrix differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Useful links:\n",
    "* [Matrix computation](http://www.machinelearning.ru/wiki/images/2/2a/Matrix-Gauss.pdf)\n",
    "* [Matrix calculus](http://www.atmos.washington.edu/~dennis/MatrixCalculus.pdf)"
   ],
   "metadata": {
    "collapsed": false,
    "id": "sYFpPBFLhI5_"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Attention: layout conventions\n",
    "\n",
    "Throughout this lab the __*consistent numerator layout*__ is used as described [here](https://en.wikipedia.org/wiki/Matrix_differentiation#Layout_conventions)\n",
    "\n",
    "#### [NB!]: the chosen layout differs from the layout in some linked materials, so the results may differ (but only up to matrix transpositions)\n",
    "\n",
    "Besides, we are going to use the __*bold italic*__ font for vectors and the __bold__ font for matrices, the former we are going to denote using lowercase letters with no overlines, while the latter will be denoted with uppercase letters"
   ],
   "metadata": {
    "collapsed": false,
    "id": "q_0vxAtghI6A"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sv4W0rlUhI6A"
   },
   "source": [
    "## ex. 1"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "y = \\boldsymbol{x}^T\\boldsymbol{x},  \\quad \\boldsymbol{x} \\in \\mathbb{R}^N\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "id": "adxdlFhihI6A"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8S9WqT79hI6B"
   },
   "source": [
    "$$\n",
    "\\frac{dy}{\\boldsymbol{dx}} = 2 \\boldsymbol{x}^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "6B_fe19ghI6B"
   },
   "source": [
    "Since $y$ is a scalar and $\\boldsymbol{\\mathit{x}}$ is a vector-column, the resulting derivative is a vector-row, the $i$-th element of which is equal to $\\frac{dy}{dx_i}$. Thus we get:\n",
    "\n",
    "$$\n",
    "\\left(\\frac{dy}{\\boldsymbol{dx}}\\right)_i = \\frac{dy}{dx_i} = \\frac{d\\left(\\boldsymbol{x}^T\\boldsymbol{x}\\right)}{dx_i} = \\frac{d\\left(\\sum\\limits_{k=1}^N x_k^2\\right)}{dx_i} =\n",
    "= \\frac{d\\left(x_i^2 + \\sum\\limits_{\\begin{subarray}{} k = 1, \\\\ k \\neq i \\end{subarray}}^N x_k^2\\right)}{dx_i} = \\frac{d\\left(x_i^2\\right)}{dx_i} = 2x_i\n",
    "$$\n",
    "\n",
    "Therefrom we immediately get the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name='ex-2'/>\n",
    "\n",
    "## ex. 2"
   ],
   "metadata": {
    "collapsed": false,
    "id": "CNPPdthQhI6C"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cvafTBbLhI6C"
   },
   "source": [
    "$$ y = tr(\\mathbf{AB}) \\quad \\mathbf{A},\\mathbf{B} \\in \\mathbb{R}^{N \\times N} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WL6f5eJohI6C"
   },
   "source": [
    "$$\n",
    "\\frac{dy}{\\mathbf{dA}} = \\mathbf{B}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since $y$ is a scalar and $\\mathbf{A}$ is an $N\\times N$ matrix, the resulting derivative is an $N\\times N$ matrix, the $\\{i, j\\}$-th element of which is equal to $\\frac{dy}{da_{ji}}$. Thus we get:\n",
    "\n",
    "$$\n",
    "\\left(\\frac{dy}{\\mathbf{dA}}\\right)_{ij} = \\frac{dy}{da_{ji}} = \\frac{d\\left(tr(\\mathbf{AB})\\right)}{da_{ji}} = \\frac{d\\left(\\sum\\limits_{k = 1}^N (\\mathbf{AB})_{kk}\\right)}{da_{ji}} = \\frac{d\\left(\\sum\\limits_{k = 1}^N \\sum\\limits_{m = 1}^N a_{km}b_{mk}\\right)}{da_{ji}} = \\frac{d\\left(a_{ji}b_{ij} + \\sum\\limits_{\\begin{subarray}{} (k, m) = (1, 1), \\\\ (k, m) \\neq (j, i) \\end{subarray}}^{(N, N)} a_{km}b_{mk}\\right)}{da_{ji}} = \\frac{d\\left(a_{ji}b_{ij}\\right)}{da_{ji}} = b_{ij}\n",
    "$$\n",
    "\n",
    "Therefrom we immediately get the answer."
   ],
   "metadata": {
    "collapsed": false,
    "id": "45KdAQOFhI6D"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PzCapIVhI6D"
   },
   "source": [
    "## ex. 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3DUFK1qhI6E"
   },
   "source": [
    "$$\n",
    "y = \\boldsymbol{x}^T\\mathbf{A}\\boldsymbol{c} , \\quad \\mathbf{A}\\in \\mathbb{R}^{N \\times N}, \\boldsymbol{x}\\in \\mathbb{R}^{N}, \\boldsymbol{c}\\in \\mathbb{R}^{N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMzMEIF4hI6E"
   },
   "source": [
    "$$\n",
    "\\frac{dy}{\\boldsymbol{dx}} = \\left(\\mathbf{A}\\boldsymbol{c}\\right)^T = \\boldsymbol{c}^T\\mathbf{A}^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since $y$ is a scalar and $\\boldsymbol{\\mathit{x}}$ is a vector-column, the resulting derivative is a vector-row, the $i$-th element of which is equal to $\\frac{dy}{dx_i}$. Thus we get:\n",
    "\n",
    "$$\n",
    "\\left(\\frac{dy}{\\boldsymbol{dx}}\\right)_i = \\frac{dy}{dx_i} = \\frac{d\\left(\\boldsymbol{x}^T\\mathbf{A}\\boldsymbol{c}\\right)}{dx_i} = \\frac{d\\left(\\boldsymbol{x}^T\\left(\\mathbf{A}\\boldsymbol{c}\\right)\\right)}{dx_i} = \\frac{d\\left(\\sum\\limits_{k = 1}^N x_k \\cdot\\left(\\mathbf{A}\\boldsymbol{c}\\right)_k\\right)}{dx_i} = \\frac{d\\left(x_i\\left(\\mathbf{A}\\boldsymbol{c}\\right)_i + \\sum\\limits_{\\begin{subarray}{} k = 1, \\\\ k \\neq i \\end{subarray}}^N x_k \\cdot\\left(\\mathbf{A}\\boldsymbol{c}\\right)_k\\right)}{dx_i} = \\frac{d\\left(x_i\\left(\\mathbf{A}\\boldsymbol{c}\\right)_i\\right)}{dx_i} = \\left(\\mathbf{A}\\boldsymbol{c}\\right)_i\n",
    "$$\n",
    "\n",
    "Since $\\mathbf{A}\\boldsymbol{c}$ is a vector-column, and as discussed above the derivative in question is a vector-row, we have to transpose it, immediately getting the answer."
   ],
   "metadata": {
    "collapsed": false,
    "id": "AzEWkdbchI6E"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0pSZJ9ohI6F"
   },
   "source": [
    "$$\n",
    "\\frac{dy}{\\mathbf{dA}} = \\boldsymbol{c}\\boldsymbol{x}^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "ZyZJV2PDhI6F"
   },
   "source": [
    "Let us consider two matrices: $\\mathbf{X}$ and $\\mathbf{C}$, each consisting of $N$ equal columns, which are equal to $\\boldsymbol{\\mathit{x}}$ and $\\boldsymbol{\\mathit{c}}$ respectively.\n",
    "\n",
    "We will denote the $\\{i, j\\}$-th elements of those matrices as $x_{ij}$ and $c_{ij}$ respectively (note: we use two indices, while the elements of the respective vectors are denoted using only one index, so no collision will occur). Then immediately from the definition of $\\mathbf{X}$ and $\\mathbf{C}$ we get:\n",
    "\n",
    "$$\n",
    "\\forall k, m = \\overline{1, N} \\hookrightarrow \\begin{cases} &x_{km} = x_k\\\\&c_{km} = c_k\\end{cases}\n",
    "$$\n",
    "\n",
    "Then let's consider $tr(\\mathbf{X}^T\\mathbf{A}\\mathbf{C})$:\n",
    "\n",
    "\\begin{gather*}\n",
    "tr(\\mathbf{X}^T\\mathbf{A}\\mathbf{C}) = \\sum\\limits_{k=1}^N \\left(\\mathbf{X}^T\\mathbf{A}\\mathbf{C}\\right)_{kk} = \\sum\\limits_{k=1}^N\\sum\\limits_{m=1}^N \\left(\\mathbf{X}^T\\mathbf{A}\\right)_{km} c_{mk} = \\sum\\limits_{k=1}^N\\sum\\limits_{m=1}^N \\left(\\sum\\limits_{l=1}^N \\left(\\mathbf{X}^T\\right)_{kl}a_{lm}\\right)c_{mk} = \\sum\\limits_{k=1}^N\\sum\\limits_{m=1}^N\\sum\\limits_{l=1}^N x_{lk}a_{lm}c_{mk} =\\\\= \\sum\\limits_{k=1}^N\\sum\\limits_{m=1}^N\\sum\\limits_{l=1}^N x_la_{lm}c_m = N\\cdot\\sum\\limits_{m=1}^N\\sum\\limits_{l=1}^N x_la_{lm}c_m = N\\cdot \\sum\\limits_{m=1}^N \\left(\\boldsymbol{x}^T\\mathbf{A}\\right)_m c_m = N\\cdot \\boldsymbol{x}^T\\mathbf{A}\\boldsymbol{c} = Ny\n",
    "\\end{gather*}\n",
    "\n",
    "Therefore, using the fact that:\n",
    "\n",
    "<a name=\"trace_two\"/>\n",
    "$$\n",
    "\\forall \\mathbf{P}, \\mathbf{Q} \\in \\mathbb{R}^{N \\times N} \\hookrightarrow tr(\\mathbf{PQ}) = \\sum\\limits_{k=1}^N \\left(\\mathbf{PQ}\\right)_{kk} = \\sum\\limits_{k = 1}^N\\sum\\limits_{m = 1}^N p_{km}q_{mk} = \\sum\\limits_{m = 1}^N\\sum\\limits_{k = 1}^N q_{mk}p_{km} = \\sum\\limits_{m = 1}^N \\left(\\mathbf{QP}\\right)_{mm} = tr(\\mathbf{QP}),\n",
    "$$\n",
    "\n",
    "and therefore\n",
    "\n",
    "<a name=\"trace_three\"/>\n",
    "$$\n",
    "\\forall \\mathbf{P}, \\mathbf{Q}, \\mathbf{R} \\in \\mathbb{R}^{N\\times N} \\hookrightarrow tr\\left(\\mathbf{PQR}\\right) = \\begin{cases}tr\\left(\\left(\\mathbf{PQ}\\right)\\mathbf{R}\\right) = tr\\left(\\mathbf{R}\\left(\\mathbf{PQ}\\right)\\right) = tr\\left(\\mathbf{RPQ}\\right)\\\\ tr\\left(\\mathbf{P}\\left(\\mathbf{QR}\\right)\\right) = tr\\left(\\left(\\mathbf{QR}\\right)\\mathbf{P}\\right) = tr\\left(\\mathbf{QRP}\\right)\\end{cases}\n",
    "$$\n",
    "\n",
    "we get:\n",
    "\n",
    "$$\n",
    "y = \\frac{1}{N} tr(\\mathbf{X}^T\\mathbf{A}\\mathbf{C}) = \\frac{1}{N} tr(\\mathbf{A}\\mathbf{C}\\mathbf{X}^T)\n",
    "$$\n",
    "\n",
    "Let's consider the matrix $\\mathbf{B} := \\mathbf{C}\\mathbf{X}^T$:\n",
    "\n",
    "$$\n",
    "b_{ij} = \\sum\\limits_{k=1}^N c_{ik}\\left(\\mathbf{X}^T\\right)_{kj} = \\sum\\limits_{k=1}^N c_{ik}x_{jk} = \\sum\\limits_{k=1}^N c_i x_j = Nc_ix_j,\n",
    "$$\n",
    "\n",
    "therefore, we get:\n",
    "\n",
    "$$\n",
    "\\mathbf{B} = N\\boldsymbol{\\mathit{c}}\\boldsymbol{\\mathit{x}}^T\n",
    "$$\n",
    "\n",
    "Then, using [ex. 2](#ex-2), we get:\n",
    "\n",
    "$$\n",
    "\\frac{dy}{\\mathbf{dA}} = \\frac{d\\left(\\frac{1}{N} tr(\\mathbf{AB})\\right)}{\\mathbf{dA}} = \\frac{1}{N}\\frac{d\\left(tr(\\mathbf{AB})\\right)}{\\mathbf{dA}} = \\frac{1}{N}\\mathbf{B} = \\frac{1}{N}\\cdot N\\boldsymbol{c}\\boldsymbol{x}^T = \\boldsymbol{c}\\boldsymbol{x}^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSZ6Ota4hI6G"
   },
   "source": [
    "## ex. 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gH87hmmhI6G"
   },
   "source": [
    "Classic matrix factorization example. Given matrix $\\mathbf{X}$ you need to find $\\mathbf{A}$, $\\mathbf{S}$ to approximate $\\mathbf{X}$. This can be done by simple gradient descent iteratively alternating $\\mathbf{A}$ and $\\mathbf{S}$ updates.\n",
    "$$\n",
    "J = || \\mathbf{X} - \\mathbf{AS} ||_F^2  , \\quad \\mathbf{A}\\in \\mathbb{R}^{N \\times R} , \\quad \\mathbf{S}\\in \\mathbb{R}^{R \\times M}\n",
    "$$\n",
    "$$\n",
    "\\frac{dJ}{\\mathbf{dS}} = -2\\left(\\mathbf{X} - \\mathbf{AS}\\right)^T\\mathbf{A}\n",
    "$$\n",
    "\n",
    "Let's consider two approaches towards getting the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umKC20syhI6G"
   },
   "source": [
    "### First approach (direct)\n",
    "\n",
    "Using the facts that:\n",
    "\n",
    "* $\\forall \\mathbf{P} \\in \\mathbb{R}^{N\\times N} \\hookrightarrow || \\mathbf{P} ||_F^2 = tr(\\mathbf{P}\\mathbf{P}^T)$\n",
    "* $\\forall \\mathbf{P} \\in \\mathbb{R}^{N\\times N} \\hookrightarrow tr\\left(\\mathbf{P}\\right) = tr\\left(\\mathbf{P}^T\\right)$\n",
    "* $\\forall \\mathbf{P}, \\mathbf{Q} \\in \\mathbb{R}^{N\\times N} \\hookrightarrow tr\\left(\\mathbf{P} + \\mathbf{Q}\\right) = tr\\left(\\mathbf{P}\\right) + tr\\left(\\mathbf{Q}\\right)$\n",
    "* [[from ex. 3](#trace_two)]: $\\forall \\mathbf{P}, \\mathbf{Q} \\in \\mathbb{R}^{N\\times N} \\hookrightarrow tr\\left(\\mathbf{PQ}\\right) = tr\\left(\\mathbf{QP}\\right)$\n",
    "* [[from ex. 3](#trace_three)]: $\\forall \\mathbf{P}, \\mathbf{Q}, \\mathbf{R} \\in \\mathbb{R}^{N\\times N} \\hookrightarrow tr\\left(\\mathbf{PQR}\\right) = tr\\left(\\mathbf{QRP}\\right) = tr\\left(\\mathbf{RPQ}\\right)$\n",
    "\n",
    "we get:\n",
    "\n",
    "\\begin{gather}\n",
    "|| \\mathbf{X} - \\mathbf{AS} ||_F^2 = tr\\left(\\left(\\mathbf{X} - \\mathbf{AS}\\right)\\left(\\mathbf{X} - \\mathbf{AS}\\right)^T\\right) = tr\\left(\\left(\\mathbf{X} - \\mathbf{AS}\\right)\\left(\\mathbf{X}^T - \\mathbf{S}^T\\mathbf{A}^T\\right)\\right) =\\\\= tr\\left(\\mathbf{X}\\mathbf{X}^T - \\mathbf{X}\\mathbf{S}^T\\mathbf{A}^T - \\mathbf{AS}\\mathbf{X}^T + \\mathbf{AS}\\mathbf{S}^T\\mathbf{A}^T \\right) =\\\\= tr\\left(\\mathbf{X}\\mathbf{X}^T\\right) - tr\\left(\\mathbf{X}\\mathbf{S}^T\\mathbf{A}^T\\right) - tr\\left(\\mathbf{AS}\\mathbf{X}^T\\right) + tr\\left(\\mathbf{AS}\\mathbf{S}^T\\mathbf{A}^T\\right) =\\\\= tr\\left(\\mathbf{X}\\mathbf{X}^T\\right) - tr\\left(\\left(\\mathbf{AS}\\mathbf{X}^T\\right)^T\\right) - tr\\left(\\mathbf{AS}\\mathbf{X}^T\\right) + tr\\left(\\mathbf{S}\\mathbf{S}^T\\mathbf{A}^T\\mathbf{A}\\right) =\\\\= tr\\left(\\mathbf{X}\\mathbf{X}^T\\right) - 2tr\\left(\\mathbf{AS}\\mathbf{X}^T\\right) + tr\\left(\\mathbf{S}\\mathbf{S}^T\\mathbf{A}^T\\mathbf{A}\\right) =\\\\= tr\\left(\\mathbf{X}\\mathbf{X}^T\\right) - 2tr\\left(\\mathbf{S}\\mathbf{X}^T\\mathbf{A}\\right) + tr\\left(\\mathbf{S}\\mathbf{S}^T\\mathbf{A}^T\\mathbf{A}\\right),\n",
    "\\end{gather}\n",
    "\n",
    "The derivative of the first term w.r.t. $\\mathbf{S}$ is 0, because the first term doesn't depend on $\\mathbf{S}$ at all.\n",
    "\n",
    "The derivative of the second term w.r.t. $\\mathbf{S}$ is $-2\\mathbf{X}^T\\mathbf{A}$ as shown in [ex. 2](#ex-2).\n",
    "\n",
    "The derivative of the third term w.r.t. $\\mathbf{S}$ is $2\\mathbf{S}^T\\mathbf{A}^T\\mathbf{A}$. Let's show that:\n",
    "\n",
    "\\begin{gather*}\n",
    "\\left(\\frac{d\\left(tr\\left(\\mathbf{S}\\mathbf{S}^T\\mathbf{A}^T\\mathbf{A}\\right)\\right)}{\\mathbf{dS}}\\right)_{ij} = \\frac{d\\left(tr\\left(\\mathbf{S}\\mathbf{S}^T\\mathbf{A}^T\\mathbf{A}\\right)\\right)}{ds_{ji}} = \\frac{d\\left(\\sum\\limits_{k=1}^R \\left(\\mathbf{S}\\mathbf{S}^T\\mathbf{A}^T\\mathbf{A}\\right)_{kk} \\right)}{ds_{ji}}=\\\\=\\frac{d\\left(\\sum\\limits_{k=1}^R \\sum\\limits_{m=1}^R \\left(\\mathbf{S}\\mathbf{S}^T\\right)_{km}\\left(\\mathbf{A}^T\\mathbf{A}\\right)_{mk}\\right)}{ds_{ji}}= \\frac{d\\left(\\sum\\limits_{k=1}^R \\sum\\limits_{m=1}^R \\left(\\sum\\limits_{l=1}^M s_{kl}\\left(\\mathbf{S}^T\\right)_{lm}\\right)\\left(\\mathbf{A}^T\\mathbf{A}\\right)_{mk}\\right)}{ds_{ji}}=\\\\=\\frac{d\\left(\\sum\\limits_{k=1}^R \\sum\\limits_{m=1}^R \\sum\\limits_{l=1}^M s_{kl}s_{ml}\\left(\\mathbf{A}^T\\mathbf{A}\\right)_{mk}\\right)}{ds_{ji}}\n",
    "\\end{gather*}\n",
    "\n",
    "Let's divide the sum in the numerator into parts ():\n",
    "\n",
    "\\begin{align*}\n",
    "&\\begin{cases}\n",
    "\tk = j\\\\ m = j\\\\ l = i\n",
    "\\end{cases}:\\quad\n",
    "s_{ji}^2 \\left(\\mathbf{A}^T\\mathbf{A}\\right)_{jj} \\quad &(1)\\\\\n",
    "&\\begin{cases}\n",
    "\tk \\neq j\\\\ m = j\\\\ l = i\n",
    "\\end{cases}:\\quad\n",
    "\\sum\\limits_{\\begin{subarray}{} k=1\\\\ k\\neq j\\end{subarray}}^R s_{ki}s_{ji}\\left(\\mathbf{A}^T\\mathbf{A}\\right)_{jk} \\quad &(2)\\\\\n",
    "&\\begin{cases}\n",
    "\tk = j\\\\ m \\neq j\\\\ l = i\n",
    "\\end{cases}:\\quad\n",
    "\\sum\\limits_{\\begin{subarray}{} m=1\\\\ m\\neq j\\end{subarray}}^R s_{ji}s_{mi}\\left(\\mathbf{A}^T\\mathbf{A}\\right)_{mj} \\quad &(3) \\\\\n",
    "&\\begin{cases}\n",
    "\tk \\neq j\\\\ m\\neq j \\\\ l = i\n",
    "    \\end{cases}:\\quad\n",
    "\\sum\\limits_{\\begin{subarray}{} k=1\\\\ k\\neq j\\end{subarray}}^R \\sum\\limits_{\\begin{subarray}{} m=1\\\\ m\\neq j\\end{subarray}}^R s_{ki}s_{mi}\\left(\\mathbf{A}^T\\mathbf{A}\\right)_{mk} \\quad &(4) \\\\\n",
    "&\\quad l\\neq i\\quad:\\quad \\sum\\limits_{k=1}^R \\sum\\limits_{m=1}^R \\sum\\limits_{\\begin{subarray}{} l=1\\\\ l\\neq i\\end{subarray}}^M s_{kl}s_{ml}\\left(\\mathbf{A}^T\\mathbf{A}\\right)_{mk}\\quad &(5) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Differentiating w.r.t. $s_{ji}$ we get, respectively:\n",
    "\n",
    "\\begin{align*}\n",
    "(1)&: 2s_{ji}\\left(\\mathbf{A}^T\\mathbf{A}\\right)_{jj}\\\\\n",
    "(2)&: \\sum\\limits_{\\begin{subarray}{} k=1\\\\ k\\neq j\\end{subarray}}^R s_{ki}\\left(\\mathbf{A}^T\\mathbf{A}\\right)_{jk}\\\\\n",
    "(3)&: \\sum\\limits_{\\begin{subarray}{} m=1\\\\ m\\neq j\\end{subarray}}^R s_{mi}\\left(\\mathbf{A}^T\\mathbf{A}\\right)_{mj}\\\\\n",
    "(4)&: 0\\\\\n",
    "(5)&: 0\\\\\n",
    "\\end{align*}\n",
    "\n",
    "If we rename the summation index in (3) from $m$ to $k$:\n",
    "\n",
    "$$\n",
    "\\sum\\limits_{\\begin{subarray}{} k=1\\\\ k\\neq j\\end{subarray}}^R s_{ki}\\left(\\mathbf{A}^T\\mathbf{A}\\right)_{kj} = \\sum\\limits_{\\begin{subarray}{} k=1\\\\ k\\neq j\\end{subarray}}^R s_{ki}\\left(\\left(\\mathbf{A}^T\\mathbf{A}\\right)^T\\right)_{jk} = \\sum\\limits_{\\begin{subarray}{} k=1\\\\ k\\neq j\\end{subarray}}^R s_{ki}\\left(\\mathbf{A}^T\\mathbf{A}\\right)_{jk}\n",
    "$$\n",
    "\n",
    "The resulting per-element derivative therefore equals:\n",
    "\n",
    "\\begin{gather*}\n",
    "\\left(\\frac{d\\left(tr\\left(\\mathbf{S}\\mathbf{S}^T\\mathbf{A}^T\\mathbf{A}\\right)\\right)}{\\mathbf{dS}}\\right)_{ij} = 2s_{ji}\\left(\\mathbf{A}^T\\mathbf{A}\\right)_{jj} + 2\\sum\\limits_{\\begin{subarray}{} k=1\\\\ k\\neq j\\end{subarray}}^R s_{ki}\\left(\\mathbf{A}^T\\mathbf{A}\\right)_{jk} = 2\\sum\\limits_{k=1}^R s_{ki}\\left(\\mathbf{A}^T\\mathbf{A}\\right)_{jk} = \\\\ = 2\\sum\\limits_{k=1}^R \\left(\\mathbf{A}^T\\mathbf{A}\\right)_{jk} s_{ki} = 2\\left(\\mathbf{A}^T\\mathbf{A}\\mathbf{S}\\right)_{ji} = 2\\left(\\left(\\mathbf{A}^T\\mathbf{A}\\mathbf{S}\\right)^T\\right)_{ij} = 2\\left(\\mathbf{S}^T\\mathbf{A}^T\\mathbf{A}\\right)_{ij}\n",
    "\\end{gather*}\n",
    "\n",
    "Therefrom, we immediately get the answer.\n",
    "\n",
    "Now, we can calculate the derivative of $J$:\n",
    "\n",
    "$$\n",
    "\\frac{dJ}{\\mathbf{dS}} = -2\\mathbf{X}^T\\mathbf{A} + 2\\mathbf{S}^T\\mathbf{A}^T\\mathbf{A} = -2\\left(\\mathbf{X}^T - \\mathbf{S}^T\\mathbf{A}^T\\right)\\mathbf{A} = -2\\left(\\mathbf{X} - \\mathbf{AS}\\right)^T\\mathbf{A}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ct2XcwHrhI6H"
   },
   "source": [
    "### Second approach (using the chain rule)\n",
    "\n",
    "Let us consider the matrix $\\mathbf{B} := \\mathbf{AS}$. Then the initial function takes the following form:\n",
    "\n",
    "$$\n",
    "J = || \\mathbf{X} - \\mathbf{B} ||_F^2  ,\\quad\\text{where } \\mathbf{B} = \\mathbf{AS}\\in \\mathbb{R}^{N \\times M}, \\quad\\mathbf{A}\\in \\mathbb{R}^{N \\times R} , \\quad \\mathbf{S}\\in \\mathbb{R}^{R \\times M}\n",
    "$$\n",
    "\n",
    "Then we can calculate the derivative of $J$ w.r.t. $\\mathbf{B}$, using an approach, similar to the first one.\n",
    "\n",
    "First we get:\n",
    "\n",
    "\n",
    "\\begin{gather*}\n",
    "|| \\mathbf{X} - \\mathbf{B} ||_F^2 = tr\\left(\\left(\\mathbf{X} - \\mathbf{B}\\right)\\left(\\mathbf{X} - \\mathbf{B}\\right)^T\\right) = tr\\left(\\left(\\mathbf{X} - \\mathbf{B}\\right)\\left(\\mathbf{X}^T - \\mathbf{B}^T\\right)\\right) =\\\\= tr\\left(\\mathbf{X}\\mathbf{X}^T - \\mathbf{X}\\mathbf{B}^T - \\mathbf{B}\\mathbf{X}^T + \\mathbf{B}\\mathbf{B}^T\\right) =\\\\= tr\\left(\\mathbf{X}\\mathbf{X}^T\\right) - tr\\left(\\mathbf{X}\\mathbf{B}^T\\right) - tr\\left(\\mathbf{B}\\mathbf{X}^T\\right) + tr\\left(\\mathbf{B}\\mathbf{B}^T\\right) =\\\\= tr\\left(\\mathbf{X}\\mathbf{X}^T\\right) - tr\\left(\\left(\\mathbf{B}\\mathbf{X}^T\\right)^T\\right) - tr\\left(\\mathbf{B}\\mathbf{X}^T\\right) + tr\\left(\\mathbf{B}\\mathbf{B}^T\\right) = \\\\ = tr\\left(\\mathbf{X}\\mathbf{X}^T\\right) - 2tr\\left(\\mathbf{B}\\mathbf{X}^T\\right) + tr\\left(\\mathbf{B}\\mathbf{B}^T\\right)\n",
    "\\end{gather*}\n",
    "\n",
    "The derivative of the first term w.r.t. $\\mathbf{B}$ is 0, because the first term doesn't depend on $\\mathbf{B}$ at all.\n",
    "\n",
    "The derivative of the second term w.r.t. $\\mathbf{B}$ is $-2\\mathbf{X}^T$ as shown in [ex. 2](#ex-2).\n",
    "\n",
    "The derivative of the third term w.r.t. $\\mathbf{B}$ is $2\\mathbf{B}^T$. Let's show that:\n",
    "\n",
    "\\begin{gather*}\n",
    "\\left(\\frac{d\\left(tr\\left(\\mathbf{B}\\mathbf{B}^T\\right)\\right)}{\\mathbf{dB}}\\right)_{ij} = \\frac{d\\left(tr\\left(\\mathbf{B}\\mathbf{B}^T\\right)\\right)}{db_{ji}} = \\frac{d\\left(\\sum\\limits_{k=1}^N \\left(\\mathbf{B}\\mathbf{B}^T\\right)_{kk}\\right)}{db_{ji}} = \\frac{d\\left(\\sum\\limits_{k=1}^N\\sum\\limits_{m=1}^M b_{km}\\left(\\mathbf{B}^T\\right)_{mk}\\right)}{db_{ji}} = \\\\ = \\frac{d\\left(\\sum\\limits_{k=1}^N\\sum\\limits_{m=1}^M b_{km}^2\\right)}{db_{ji}} = \\frac{d\\left(b_{ji}^2 + \\sum\\limits_{\\begin{subarray}{} (k, m) = (1, 1), \\\\ (k, m) \\neq (j, i) \\end{subarray}}^{(N, M)} b_{km}^2\\right)}{db_{ji}} = \\frac{d\\left(b_{ji}^2\\right)}{db_{ji}} = 2b_{ji} = 2\\left(\\mathbf{B}^T\\right)_{ij}\n",
    "\\end{gather*}\n",
    "\n",
    "Therefrom we immediately get the answer.\n",
    "\n",
    "Thus the derivative of $J$ w.r.t. $\\mathbf{B}$ equals:\n",
    "\n",
    "$$\n",
    "\\frac{dJ}{\\mathbf{dB}} = -2\\mathbf{X}^T + 2\\mathbf{B}^T = -2\\left(\\mathbf{X} - \\mathbf{B}\\right)^T\n",
    "$$\n",
    "\n",
    "Now let's calculate the derivative of $\\mathbf{B}$ w.r.t. $\\mathbf{S}$. Since $\\mathbf{B}$ is an $N\\times M$ matrix and $\\mathbf{S}$ is an $R\\times M$ matrix, the resulting derivative in the chosen layout will be an $N\\times M\\times M\\times R$ tensor, and:\n",
    "\n",
    "$$\n",
    "\\left(\\frac{\\mathbf{dB}}{\\mathbf{dS}}\\right)_{ijkm} = \\frac{db_{ij}}{ds_{mk}} = \\frac{d\\left(\\sum\\limits_{l=1}^R a_{il}s_{lj}\\right)}{ds_{mk}}\n",
    "$$\n",
    "\n",
    "In case $j\\neq k$ the expression above equals $0$, because the sum in the numerator does not contain $s_{mk}$.\n",
    "\n",
    "In case $j = k$ the expression above equals $a_{im}$, because the only term from the sum in the numerator containing $s_{mk}$ is $a_{im}s_{mk}$.\n",
    "\n",
    "Therefore, we get:\n",
    "\n",
    "$$\n",
    "\\left(\\frac{\\mathbf{dB}}{\\mathbf{dS}}\\right)_{ijkm} = \\begin{cases}\n",
    "a_{im}&\\text{, if } j = k\\\\\n",
    "0&\\text{, else} \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Finally, for the derivative in question we get:\n",
    "\n",
    "$$\n",
    "\\frac{dJ}{\\mathbf{dS}} = \\frac{dJ}{\\mathbf{dB}}\\frac{\\mathbf{dB}}{\\mathbf{dS}},\n",
    "$$\n",
    "\n",
    "where by multiplication a tensor convolution is implied. Thus, we get:\n",
    "\n",
    "$$\n",
    "\\left(\\frac{dJ}{\\mathbf{dS}}\\right)_{km} = \\sum\\limits_{i=1}^N\\sum\\limits_{j=1}^M\\left(\\frac{dJ}{\\mathbf{dB}}\\right)_{ji}\\left(\\frac{\\mathbf{dB}}{\\mathbf{dS}}\\right)_{ijkm} = \\left[\\begin{gather*} \\text{if } j\\neq k,\\\\ \\text{then } \\left(\\frac{\\mathbf{dB}}{\\mathbf{dS}}\\right)_{ijkm} = 0\\end{gather*}\\right] = \\sum\\limits_{i=1}^N\\left(\\frac{dJ}{\\mathbf{dB}}\\right)_{ki}\\left(\\frac{\\mathbf{dB}}{\\mathbf{dS}}\\right)_{ikkm} =\\\\= \\sum\\limits_{i=1}^N \\left(-2\\left(\\mathbf{X} - \\mathbf{B}\\right)^T\\right)_{ki}a_{im} = \\left(-2\\left(\\mathbf{X} - \\mathbf{B}\\right)^T\\mathbf{A}\\right)_{km}\n",
    "$$\n",
    "\n",
    "Therefrom we immediately get the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "id": "LKsqCFCthI6I"
   },
   "source": [
    "## 2. kNN questions\n",
    "Here come the questions from the assignment0_01. Please, refer to the assignment0_01 to get the context of the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThQKvp4IhI6I"
   },
   "source": [
    "### Question 1\n",
    "\n",
    "Notice the structured patterns in the distance matrix, where some rows or columns are visible brighter. (Note that with the default color scheme black indicates low distances while white indicates high distances.)\n",
    "\n",
    "- What in the data is the cause behind the distinctly bright rows?\n",
    "- What causes the columns?\n",
    "\n",
    "*Your Answer:*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XvXLJFH5hI6J"
   },
   "source": [
    "### Question 2\n",
    "\n",
    "We can also use other distance metrics such as L1 distance.\n",
    "For pixel values $p_{ij}^{(k)}$ at location $(i,j)$ of some image $I_k$,\n",
    "\n",
    "the mean $\\mu$ across all pixels over all images is $$\\mu=\\frac{1}{nhw}\\sum_{k=1}^n\\sum_{i=1}^{h}\\sum_{j=1}^{w}p_{ij}^{(k)}$$\n",
    "And the pixel-wise mean $\\mu_{ij}$ across all images is\n",
    "$$\\mu_{ij}=\\frac{1}{n}\\sum_{k=1}^np_{ij}^{(k)}.$$\n",
    "The general standard deviation $\\sigma$ and pixel-wise standard deviation $\\sigma_{ij}$ is defined similarly.\n",
    "\n",
    "Which of the following preprocessing steps will not change the performance of a Nearest Neighbor classifier that uses L1 distance? Select all that apply.\n",
    "1. Subtracting the mean $\\mu$ ($\\tilde{p}_{ij}^{(k)}=p_{ij}^{(k)}-\\mu$.)\n",
    "2. Subtracting the per pixel mean $\\mu_{ij}$  ($\\tilde{p}_{ij}^{(k)}=p_{ij}^{(k)}-\\mu_{ij}$.)\n",
    "3. Subtracting the mean $\\mu$ and dividing by the standard deviation $\\sigma$.\n",
    "4. Subtracting the pixel-wise mean $\\mu_{ij}$ and dividing by the pixel-wise standard deviation $\\sigma_{ij}$.\n",
    "5. Rotating the coordinate axes of the data.\n",
    "\n",
    "*Your Answer:*\n",
    "\n",
    "\n",
    "*Your Explanation:*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZnKaE2lhI6J"
   },
   "source": [
    "## Question 3\n",
    "\n",
    "Which of the following statements about $k$-Nearest Neighbor ($k$-NN) are true in a classification setting, and for all $k$? Select all that apply.\n",
    "1. The decision boundary (hyperplane between classes in feature space) of the k-NN classifier is linear.\n",
    "2. The training error of a 1-NN will always be lower than that of 5-NN.\n",
    "3. The test error of a 1-NN will always be lower than that of a 5-NN.\n",
    "4. The time needed to classify a test example with the k-NN classifier grows with the size of the training set.\n",
    "5. None of the above.\n",
    "\n",
    "*Your Answer:*\n",
    "\n",
    "\n",
    "*Your Explanation:*\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mipt",
   "language": "python",
   "name": "mipt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "colab": {
   "provenance": [],
   "collapsed_sections": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}